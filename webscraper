import argparse
import hashlib
import logging
import re
from pathlib import Path
from urllib.parse import urlparse

import justext
import requests
from bs4 import BeautifulSoup
from trafilatura import fetch_url, extract

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler('scraper.log'), logging.StreamHandler()]
)
logger = logging.getLogger('gold_standard_scraper')

def get_clean_domain(url: str) -> str:
    """Extract sanitized domain name from URL"""
    parsed = urlparse(url)
    domain = parsed.netloc.replace('www.', '').split(':')[0]
    return re.sub(r'[^a-zA-Z0-9]+', '-', domain)

def generate_filename(url: str, content: str) -> str:
    """Generate unique filename using domain and content hash"""
    domain = get_clean_domain(url)
    content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
    return f"{domain}-{content_hash}.txt"

def fetch_html(url: str) -> str:
    """Fetch HTML content with proper headers and error handling"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                      'AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': 'en-US,en;q=0.5'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        logger.error(f"Request failed: {e}")
        raise

def extract_body_content(html: str, url: str) -> str:
    """
    Extract main body content using multiple fallback strategies:
    1. Trafilatura (optimized for modern pages)
    2. Justext (boilerplate removal)
    3. BeautifulSoup fallback
    """
    # Strategy 1: Trafilatura
    trafilatura_content = extract(
        html,
        include_links=False,
        include_tables=False,
        output_format='text'
    )
    if trafilatura_content and len(trafilatura_content) > 300:
        return trafilatura_content.strip()

    # Strategy 2: Justext
    paragraphs = justext.justext(html, justext.get_stoplist("English"))
    justext_content = '\n'.join(
        p.text for p in paragraphs if not p.is_boilerplate
    )
    if justext_content and len(justext_content) > 300:
        return justext_content.strip()

    # Strategy 3: BeautifulSoup fallback
    logger.warning("Using fallback extraction method")
    soup = BeautifulSoup(html, 'lxml')
    
    # Remove non-content elements
    for element in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):
        element.decompose()
    
    # Prioritize <article> and <main> tags
    body = soup.find('article') or soup.find('main') or soup.body
    return body.get_text(separator='\n', strip=True) if body else ""

def save_content(content: str, filename: str, output_dir: Path):
    """Save content to file following gold standard conventions"""
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / filename
    
    with output_path.open('w', encoding='utf-8') as f:
        f.write(content)
    logger.info(f"Saved content to {output_path}")

def process_url(url: str, output_dir: Path):
    """Full processing pipeline for a single URL"""
    try:
        logger.info(f"Processing: {url}")
        html = fetch_html(url)
        content = extract_body_content(html, url)
        
        if not content:
            logger.warning(f"No content extracted from {url}")
            return
            
        filename = generate_filename(url, content)
        save_content(content, filename, output_dir)
    except Exception as e:
        logger.exception(f"Failed to process {url}: {e}")

def main():
    parser = argparse.ArgumentParser(
        description='Gold Standard Web Scraper for Model Training Data'
    )
    parser.add_argument('urls', nargs='+', help='URL(s) to scrape')
    parser.add_argument(
        '--output',
        default='scraped_data',
        help='Output directory (default: scraped_data)'
    )
    args = parser.parse_args()

    output_dir = Path(args.output)
    logger.info(f"Starting scraping with output directory: {output_dir}")
    
    for url in args.urls:
        process_url(url, output_dir)
    
    logger.info("Scraping completed")

if __name__ == "__main__":
    main()
